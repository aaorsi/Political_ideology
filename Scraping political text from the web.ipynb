{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook scrapes web articles from two political websites: Salon (liberal) and Townhall (conservative). Text data is stored as a .csv file, including the source url of each article. This data is intended to be used as training in a political ideology text classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salon\n",
    "## A liberal website\n",
    "\n",
    "Political articles are scraped from the list of 'Trending' articles. That site links to a list of about 20 articles and also to a subsequent Page with more articles. the code below scrapes the text from each of those articles iteratively. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list0_url = 'https://www.salon.com/category/news-and-politics?sort=trending&type=all'\n",
    "\n",
    "article_count = 0 # Let's stop after a reasonable number, I guess\n",
    "page_count = 1\n",
    "\n",
    "articles = {'url':[], 'text':[]}\n",
    "\n",
    "while article_count < 20000: \n",
    "    if article_count == 0:\n",
    "        list_url = list0_url\n",
    "    \n",
    "    print('scraping from %s'%list_url)\n",
    "    r = requests.get(list_url)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    links = []\n",
    "# get links in the page    \n",
    "    for link in soup.find_all('a'):\n",
    "        ilink = link.get('href')\n",
    "        if ilink is not None:\n",
    "            if ilink.startswith('/201'): # This hopefully points to 2017, 2018, so on, and it links to actual articles\n",
    "                links.append(\"https://www.salon.com%s\"%ilink)\n",
    "            if 'pagenum=%d'%(page_count+1) in ilink:\n",
    "                list_url = \"https://www.salon.com%s\"%ilink # This should correspond to the next page with articles\n",
    "    for article in links:\n",
    "        print('article from %s'%article)\n",
    "        r = requests.get(article)\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        articles['text'].append(soup.find_all('article'))\n",
    "        articles['url'].append(article)\n",
    "        article_count += 1\n",
    "    page_count += 1\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(articles['url']))\n",
    "import pandas as pd\n",
    "salon_df = pd.DataFrame.from_dict(articles)\n",
    "salon_file = 'salon_data.csv'\n",
    "\n",
    "salon_df.to_csv(salon_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Townhall\n",
    "## A conservative news and opinion site\n",
    "Here I scrap text from columnists sorted by date. The script scans the columinsts page for a given week and goes back in time one week at a time. There are on average around 90 articles per page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "x0 = datetime.datetime(2019, 7, 21) #- datetime.timedelta(days=7)\n",
    "\n",
    "list0 = 'https://townhall.com/columnists/date/'\n",
    "\n",
    "article_count = 0 # Let's stop after a reasonable number, I guess\n",
    "page_count = 1\n",
    "weeks = 0\n",
    "\n",
    "townhall = {'url':[], 'text':[]}\n",
    "\n",
    "while article_count < 5000: \n",
    "    xx = x0 - datetime.timedelta(days = 7*weeks)\n",
    "    list_url = \"%s%s\"%(list0, xx.strftime(\"%Y/%m/%d\"))\n",
    "    print('scraping from %s'%list_url)\n",
    "    r = requests.get(list_url)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        ilink = link.get('href')\n",
    "        if ilink is not None:\n",
    "            if ilink.startswith('https://townhall.com/columnists'): # This hopefully points to 2017, 2018, so on, and it links to actual articles\n",
    "                links.append(ilink)\n",
    "    print ('%d articles found in page'%(len(links)/3))\n",
    "    for i in range(0,len(links),3):\n",
    "        article = links[i]\n",
    "        print('article from %s'%article)\n",
    "        r = requests.get(article)\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        townhall['text'].append(soup.find_all('p'))\n",
    "        townhall['url'].append(article)\n",
    "        article_count += 1\n",
    "    weeks += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving data\n",
    "townhall_df = pd.DataFrame.from_dict(townhall)\n",
    "townhall_file = 'townhall_data.csv'\n",
    "townhall_df.to_csv(townhall_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(townhall['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
